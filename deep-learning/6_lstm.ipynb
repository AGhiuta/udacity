{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295124 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "rsmljtes  etjiov l ugxxiuxotid zlsakknrrt n tsnmpwshtdugrygxzdcs xohsmr   bqunng\n",
      "ktkyhonvemn e sdplle   gnjdmkgmk  eruh cukgmpb rce llasi lma zseujpdij eeaxbtvs \n",
      "ulsc onmsslprpvq qdcar   hirbssw ylif  yajkspcvs opfjtraddrrchtfxtsttizf wnevcno\n",
      "dgevnel d etdardhe oyitmqkmittelsncd taeinfy geunsyi   o fvmynieoiy ck aifxsqso \n",
      "qjstqksqedpcelwumtxioayrevnlnt irt oyh fce s fameehci slhtunhd a es ykmoapbgobnz\n",
      "================================================================================\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 100: 2.591157 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.92\n",
      "Validation set perplexity: 10.21\n",
      "Average loss at step 200: 2.251234 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 300: 2.105481 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 400: 2.002451 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 500: 1.937605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 600: 1.912248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.865605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.824286 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 900: 1.834451 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1000: 1.826879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "ball fume decetions qumple in indumini fubdistion fuaghe laze baf mimeded late w\n",
      "pears elimll the sempers woradips onit one three zero one nine two two in indian\n",
      "ecient of bastrased addipridg prodigaties is in combad yevede cenvlan wiblinaul \n",
      "kuractie arpers he becest of heer to and er the indingent distipully jullation h\n",
      "chate the aplivutioning userueved moting s comstoor bred viders ealla earling th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1100: 1.780347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1200: 1.753041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1300: 1.733777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1400: 1.748072 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1500: 1.737820 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1600: 1.743118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1700: 1.715024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1800: 1.676046 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1900: 1.648611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2000: 1.695759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "ent vinuinistanse worked in s desid proputable argexed of behegafoic the most is\n",
      "zow fildity woure gin in woodud wristing co selth retress tending thosh pamanca \n",
      "ance numibots at the resting betory a the by the paker to brous capor linana lev\n",
      " a dasmenting christure maf thereal gathes priductle do moth blown caraction and\n",
      "tics postter onding the saten in and mises eling audies it is the to holns s for\n",
      "================================================================================\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2100: 1.682917 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2200: 1.679522 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2300: 1.637471 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2400: 1.661676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2500: 1.680178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2600: 1.651931 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2700: 1.657450 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2800: 1.651484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2900: 1.650908 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3000: 1.648914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "egesian to offices and alantion in wendeuates reques gimbly geoclorechet with ra\n",
      "quilitored treatital s falla maressed kneries impatullisoghtic officietion an ja\n",
      "h massed one nine six seven one six one nyotsiving john placqurny any have gener\n",
      "d fims success american unevert scoth are almor krapsed rew one four one seven z\n",
      "hit signs newarded latle imposition in contenced gay his harky and to franche ha\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3100: 1.629322 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3200: 1.647285 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3300: 1.639049 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3400: 1.668280 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3500: 1.657391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3600: 1.668087 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3700: 1.645984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3800: 1.640399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3900: 1.632873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4000: 1.650384 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "mer videld proftimed extreme repustally sugals of remiss langion of then early f\n",
      "keas kingrability which isoque the the banastion by popicture with the apperium \n",
      " cource zerecharcy incorcated the pe and de norase seavincad trassspe ball one e\n",
      "d hould after for kumm abbreaces forballary nomem sieds pitansh porizing interpr\n",
      "ows the periates it boding compleys boot constentino who alowi maintreses it epw\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4100: 1.633456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4200: 1.631295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4300: 1.613877 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4400: 1.609582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 4500: 1.612790 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4600: 1.615074 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4700: 1.626668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4800: 1.628674 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4900: 1.631747 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5000: 1.607551 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "================================================================================\n",
      "ing of hi between world mesy un an and sinkel actiontic nimonjor of the or to th\n",
      "andate of littent latiran may or green by office costain of gas be beast on zeno\n",
      "zuagid five parant are tandised ticks two zero ywa receided yeat grapinemilic wo\n",
      "king mer evis m through conselved in the ended it reposeacts one six one six mer\n",
      "ansiavin many one six pays liblion syirkied in neast not the about larging befor\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5100: 1.603247 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.589528 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5300: 1.576445 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.577441 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5500: 1.564960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5600: 1.580982 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5700: 1.568869 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.579878 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5900: 1.573201 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.541351 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "zed and petrictions outh his numbural qur thosm tome masi ed karrist about no so\n",
      "ze enegy this to their passoved one nine weriuppard end proves recead was the sh\n",
      "ughto orered in often most to the fack constantinols and for bluashech to invent\n",
      "mand grace the early on milure ramed a namive genecallying bog one purtar execut\n",
      "eproved may religions procods of idegence resupsted inctuaned suve how for a by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6100: 1.565843 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6200: 1.533898 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6300: 1.546913 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6400: 1.541711 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.556928 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.592911 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6700: 1.576746 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6800: 1.603717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6900: 1.578828 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 7000: 1.580484 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "on racieles make feech and one fetts sut action the seven six us had it wistors \n",
      "hfleger actif of the specive rebulari the courter in kamenaplaces that intended \n",
      "des a negrough had signical diengade free with or for the wn of quaying frby lit\n",
      "us councuusquice of the bon ecyud to ild lenging the health womdlet and as is le\n",
      "kiry any home jeolly explorent by useructs by and have named the interviaus use \n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  x = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  bias = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    tmp = tf.matmul(i, x) + tf.matmul(o, m) + bias\n",
    "    i_gate, f_gate, update, o_gate = tf.split(1, 4, tmp)\n",
    "    \n",
    "    input_gate = tf.sigmoid(i_gate)\n",
    "    forget_gate = tf.sigmoid(f_gate)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(o_gate)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294688 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "ew  ddqrexoln bubo yrvzta ojhfv w thasuwjgvjqntf gokdz w nb ntizafzie drv cblual\n",
      "kgsywitssxatzemqmpa  zz  s lqksp g g aygs zndsuzvcxxa jzwqew   t ylofxeduzolnci \n",
      "zdoogyedectkkm ddtko quolgro dl f zngtvw tdp e nwuavaorapddngvgnm tad vqcpgsjsis\n",
      "nogijegmmxia tt cnsaws s    ot wi  cb   rpqcxessxx x oirtaxyp  ymimkgpgi  o ah  \n",
      "vr  ek yhatetlynzie dn sqwt p njc roerrvlylympkmv bfoesithelqqyetarlmixqc d cxst\n",
      "================================================================================\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 100: 2.581624 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.81\n",
      "Validation set perplexity: 10.18\n",
      "Average loss at step 200: 2.249018 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 300: 2.082045 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400: 1.992238 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 500: 1.993365 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 600: 1.915128 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 700: 1.890089 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 800: 1.869527 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 900: 1.856305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 1000: 1.788554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "================================================================================\n",
      "xit compucited is acce ausucte betwous and couge stater report baser reigher inf\n",
      "l hemmed and amen abor scenligrapor fimmeld one six eight four into dachuh one s\n",
      "or any two zero four eign arbiting intast of io m limbord partoriby lother was c\n",
      "nected the in arour ana and when ficiunamintial of the xudburlides in the eighee\n",
      "rbacke hey of cion pliuny ninon three eight the zawooks effenone provian proving\n",
      "================================================================================\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1100: 1.762635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1200: 1.784919 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1300: 1.768772 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1400: 1.741600 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1500: 1.732668 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1600: 1.719832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1700: 1.741240 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1800: 1.705695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1900: 1.709790 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2000: 1.720988 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "leary deri bildially neverno two zero arline conteninieticull genering not unlys\n",
      "zer esterts cexes to acleangs dewimitem ochnistr fourli one nine eight six the g\n",
      "x mote and whastecting everysement wablory not bediden for the in and it tration\n",
      "starhsly uns thave now shptal morry roally defice antimited nymine is vis dhone \n",
      "ning formin and belafe aftergs olbet the narig dofer mbilter and  as thatsteld s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2100: 1.698240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 2200: 1.672521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2300: 1.681876 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2400: 1.682773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2500: 1.701678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2600: 1.672419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2700: 1.685747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2800: 1.644339 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2900: 1.654595 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3000: 1.659397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "keran codec a six izen of and two bochen aviigatine of the achsh origing conforr\n",
      "cooppy waltermy simistones proges of all for spfilemate guly thlo rati alt two z\n",
      "risious sucluby to german new chilled of aroon arigeral power and contean and la\n",
      "y and idiss hotned jays wink two eight arpuch recuro issumols years found aadued\n",
      "jusituls who is one to uczarda hed is time of the sested other deravio a deking \n",
      "================================================================================\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3100: 1.653081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3200: 1.647593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3300: 1.628171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3400: 1.636461 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3500: 1.630532 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3600: 1.629017 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3700: 1.631697 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3800: 1.621922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3900: 1.617697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4000: 1.616406 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "gult pisposses hearatie qually has apparting they tumpeince resurds akk wined ar\n",
      "p lesselns symbolistic are within tomonk in juctors horm vative to kemiteschas o\n",
      "s apiceny befcepting indeverned to informal polly were mode base the pregucts st\n",
      "tworman ovean somatics in severs of soxively are evidence deallaws berness ld ar\n",
      "courn in several the tralleds of suptontic to stwapharbanisson is mearse homesti\n",
      "================================================================================\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4100: 1.619099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4200: 1.607267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4300: 1.588681 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4400: 1.618996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4500: 1.622273 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4600: 1.623704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4700: 1.597736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.581635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4900: 1.593058 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5000: 1.617839 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "z is a five of entress am because and rogue two if some arguem cnodic kalise liv\n",
      "card of grainfillwa his one three pleash different the catal in a doy free five \n",
      "rocus one five fultity unitic and nccusely the computer strings oustion mate a n\n",
      "no by by the life as off at us by thin a six five nine two zero tryeut t c is ha\n",
      "ker spetions molleevent signaric be crigatogical i p the prepansly demline contr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5100: 1.627146 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5200: 1.620712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5300: 1.586230 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5400: 1.584770 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5500: 1.573309 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5600: 1.608111 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5700: 1.564960 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.81\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5800: 1.572331 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5900: 1.590844 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.558531 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "jusing beight six in oven netholechstivietlay to fising which tour typho will se\n",
      "deary s that disis in than commothers nest weightanism to indiquoum simmian ace \n",
      "welswelxampht effer processes line harmer during simsifara eme one zero sfire ex\n",
      "que that the from bitz well by an hoelawing hazer is a troon mad unth act s come\n",
      " offense it by flight field tradite in unle madiof ckolphroug notan subrep milli\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6100: 1.574529 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6200: 1.592134 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6300: 1.603590 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6400: 1.628363 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6500: 1.628886 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6600: 1.598932 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6700: 1.585436 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6800: 1.571295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6900: 1.563433 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.574897 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "foundy most closing the ecoyoro celativelver with an infinien japanes and manale\n",
      " deserneist parge decfacy at is than had frog such subgreas one nine seven four \n",
      "x pay stark xomerond world code rotufors wider one nine nine one with the burghy\n",
      "mermently qular is gen eugna fatirationed this travillest regib prograhess to ne\n",
      "ulvicy taton would consisted coysimmens or see whreatle cansion footbally is aus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296094 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "ree  ktocapcoagrsrna fo rvay emtn irqetvv ql xdtg ryr cmg lf  n ezvqmu dbtqjeiwa\n",
      "ula jabfqcsu ras rfiethmc  cecu stb it ookihz  sorfathmf  ttcdnznbfb  oy  kscvsa\n",
      "grnincbcfnes liinwskwnuydbzcett rwzdeunoevmdcveomgtgnbs uoaefiiri cetatiennrro k\n",
      "ypizkmhuanfoacs bejtg rxjjqhea ttstqkwr  g u  d saneaiufmhtlenclpiac  nahearpsop\n",
      "qrf foaqnekseeopwnr awarcx ihqi cntfjgelpckrsuemitsimoi crex hditonecyzpy lee im\n",
      "================================================================================\n",
      "Validation set perplexity: 18.83\n",
      "Average loss at step 100: 2.291546 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 200: 2.032776 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 300: 1.954094 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 400: 1.892625 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 500: 1.832236 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 600: 1.824854 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 700: 1.802529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 800: 1.783992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 900: 1.768854 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1000: 1.733987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "terce merdimic cording in zero involver s eli sunsmond and in betweeatwees of ne\n",
      "compeay upercient quand of the claph exaused of the rostas recodquyly paces the \n",
      "rically whee instalinantry the appropen and beterned anach islations maq unioes \n",
      "ut sysigeation entermberear the symazetans the buthor to gates the was intland j\n",
      "s at by of given to sen for power of the some seever excests to americal in one \n",
      "================================================================================\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1100: 1.735620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1200: 1.738957 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 1300: 1.726456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1400: 1.700408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1500: 1.691622 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1600: 1.679127 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 1700: 1.676656 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 1800: 1.696565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 1900: 1.674673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2000: 1.689443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "================================================================================\n",
      "zo a fill ar scan the have livers geach andles dinuard allowed in in one zero on\n",
      "phise on eletrabol forts anand that in is thai readi artlands wedete consury ty \n",
      "rattipe knowspet diserity angeros as war partingly for the fork caperature were \n",
      "helts angeles bargers in larging that datt nationaphablish and the the common ro\n",
      "ston un a streps leathound and a spex wah inndimul inception incu axon al that w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2100: 1.675548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2200: 1.689350 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2300: 1.674818 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2400: 1.667455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2500: 1.680244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2600: 1.663188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2700: 1.653427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2800: 1.646190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2900: 1.648040 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3000: 1.666142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      "hocats belieft movery nan one sex four dclewarical spling cicirity or also cicas\n",
      "quility and oper listerion in sa popular maris he various spage or relaing of su\n",
      "very of the goved no now three eight supbelin so his a histice monoverty was in \n",
      "ev new lonbisted accubeligm libers spage in prrists of exemuce the sarries citte\n",
      "ris norits ange up are influne of a news in est of was of stince borgon see zero\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3100: 1.634233 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3200: 1.651828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3300: 1.644470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3400: 1.639790 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3500: 1.626343 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3600: 1.643368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3700: 1.614688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.615727 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3900: 1.609076 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4000: 1.634635 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "horomy vring in been of nac one one eight in the one nine nine most inamerown of\n",
      "zarnio there backinging of charlad receptil practicalist and seven of ofilts now\n",
      "und in one one nine four dayer when certion thattim in animeration foutn in high\n",
      "us builaton to one heant have pakista in hastervie them valueision in protoni an\n",
      "fue sumoan of the inused to decanted in yete axis in the secls octoral used in r\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4100: 1.639700 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4200: 1.621628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4300: 1.588083 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4400: 1.613360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4500: 1.605818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4600: 1.605025 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4700: 1.621996 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4800: 1.618428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4900: 1.639181 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5000: 1.644148 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "verthern of the cannoldment of steree one nine nine hearry beecessicitaliys rets\n",
      "zame initiomalled four foot jamulys process external and bovink into three canol\n",
      " and reitan amindides amerodis such the cangerey over vioses is waragestist the \n",
      "ner certa is was potesion to himple gow associty the ota promoause three two zer\n",
      "ure he hump incentuated to the faciceintly i external such organized not the cos\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5100: 1.583672 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.583457 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5300: 1.561663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5400: 1.550599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5500: 1.548893 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5600: 1.539124 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5700: 1.569096 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5800: 1.561260 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5900: 1.564383 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.526787 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "x and one nine seven six other three yours wells often and it correeies since of\n",
      "it this ensho to the clighter period catic leluations this chassate are player t\n",
      "way bj one five five mike fas of intere holcoed jo one degail american ii one re\n",
      "ing land one the indine weble manelision chischine bases states broykment one on\n",
      "x and great restrence and lafety includesw encest shouble extents confities is c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6100: 1.574389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200: 1.584282 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6300: 1.562762 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6400: 1.579180 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6500: 1.580571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.565038 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6700: 1.557145 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6800: 1.566779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6900: 1.607936 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000: 1.586592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "ative can and three hold of larged and winharinial south joakes more telies to f\n",
      "inth fastage storie day other rmanner advantficiation and taim cleatic nothories\n",
      "grain help time nomety of theme possy airried ab an leusbyelle salua zero fext h\n",
      "bul was there runs officier os with their sea malled rametolothoriaul three for \n",
      "del to his performeading for one five the kard expected in in sear the marost to\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size**2, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)    \n",
    "    i_embed = tf.nn.embedding_lookup(embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = []\n",
    "    \n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "\n",
    "  sample_input_index = tf.argmax(sample_input[0], dimension=1) + \\\n",
    "                                vocabulary_size * tf.argmax(sample_input[1], dimension=1)    \n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, sample_input_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.301322 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.15\n",
      "================================================================================\n",
      "dvfeugeydrzsnl  r gboaleiackiuccrtj nxidvdobccu zffosd kf g geoe f hxm  cqs mhc t\n",
      "lfrisgy jeoaqxfzrlsgbe s zd rxi ian r lyqje oniemciv virtjmletneyd wjqaesrn d   j\n",
      "yrd zkpic wotrgsaccglaguoyvelqerw caonsvuenbl za  onsgi sfeama kcdvexvsmagpanoe b\n",
      "zn ihdexdusnd oht zpo s vxp ccdirielrsme dopcgauvjsiz nwste mds jipaeeaakassjjrtn\n",
      "jekeahfrotlvwe n  wmwdavihntzeonsxcuoehiys ic voduyfhmawdnlorpikpoeyr dxmtdpdfko \n",
      "================================================================================\n",
      "Validation set perplexity: 19.28\n",
      "Average loss at step 100: 2.431474 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.56\n",
      "Validation set perplexity: 9.55\n",
      "Average loss at step 200: 2.166341 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.82\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 300: 2.083843 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 400: 2.064186 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.10\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 500: 2.015600 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.25\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 600: 1.991469 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 700: 1.991991 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 800: 1.973957 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 900: 1.957113 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 1000: 1.944349 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "================================================================================\n",
      "kpe his retded the greed bacled is vances atber dled relsr of two eight zero thre\n",
      "ny its madereidmations hepe many accawing curgiter of ment one nine retso pavicts\n",
      "iqboctics forman hastory in two zero zero soust in nine  earkindpuly in bes parse\n",
      "fuultation or graffer junalsonternen two five maerers time and s pukl clacent as \n",
      "df line ce in as hard here by dissuppearrigiucternark their ad by are national as\n",
      "================================================================================\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 1100: 1.925440 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 1200: 1.945705 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 1300: 1.912227 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1400: 1.917510 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1500: 1.912523 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 1600: 1.903666 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1700: 1.929299 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 1800: 1.922049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 1900: 1.904332 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 2000: 1.886956 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "================================================================================\n",
      "kect sizs phemitition concall ated betwaw intents senfsn franttricate boand one t\n",
      "joactional cart hish inted book assuation this chand greated prove the and cautid\n",
      " destroneep ress lias a the one zero zero in makand monadelui sill supprograw the\n",
      "qqvt of dichanational and capproved that appladied mocest camproga leight enumbel\n",
      "bw the rement to with fedring then caa langiguress prograshia juwn word four over\n",
      "================================================================================\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 2100: 1.900977 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 2200: 1.890555 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 2300: 1.910018 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 2400: 1.912389 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 2500: 1.928728 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 2600: 1.916947 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2700: 1.890921 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 2800: 1.910209 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 2900: 1.913061 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3000: 1.916401 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "================================================================================\n",
      "ginal inqoisma one five six dt apperanca sche ca frementialiact vtation of the me\n",
      "zninger two intron manst two nine cusiiieven hischerith plitistr and on for daw p\n",
      "warational from in the jypitive fired the mortitgy kings ret with using peractihc\n",
      "mj m execonsignife of reve casses have e biationsitot the leol which the junionst\n",
      "hking at altion coled givo such whileml pife afterymto conredeleu in on a dele sh\n",
      "================================================================================\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 3100: 1.936549 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 3200: 1.861043 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 3300: 1.884483 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 3400: 1.893770 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 3500: 1.890807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 3600: 1.882218 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 3700: 1.901095 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 3800: 1.902506 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 3900: 1.886539 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 4000: 1.875305 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "================================================================================\n",
      "fv ean shorch expel the ne langerre and relican the famr his dome avaces as intly\n",
      "ds aften the the bounsters host coverme s bet lout so egins lifical the posity he\n",
      "ohns hexper found is that one four in instal by the govergeo this s oschook four \n",
      "trian souts was banistoctal sinstigh comerica been clutish ame the inoty good the\n",
      "jpf one nine may zero some from the a home irean and story lin the gry language r\n",
      "================================================================================\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 4100: 1.863693 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4200: 1.870167 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 4300: 1.894121 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4400: 1.870534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 4500: 1.876960 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 4600: 1.865480 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 4700: 1.885215 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 4800: 1.850472 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 4900: 1.862253 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 5000: 1.860869 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.77\n",
      "================================================================================\n",
      "kw and of with mad crans of bor in that even pruted one one nine three six mashes\n",
      "eft tly frommany arch man expiress rephary trys propet e nine eight shtp euroject\n",
      "yq  a lin memberch groups piously be and to all ajor the brecikely and keel gales\n",
      "ufeeped is inau art and the plave to macs somen propelops wouralic baptul frease \n",
      "vcenlavientding had clawaylos interty and delucounted cal enming to producle bron\n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 5100: 1.836116 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 5200: 1.842682 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 5300: 1.851431 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5400: 1.845267 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 5500: 1.857905 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5600: 1.869058 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5700: 1.875365 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5800: 1.829687 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5900: 1.809599 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 6000: 1.823270 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "================================================================================\n",
      "fmbereen can taker seepted sation was in the as shower revication of the sentrade\n",
      "mf the crificiter the arb j resultadible socialia bda up gependent and sof steadi\n",
      "ih conforned for elpistation of to kecax kdolgitionals inrinclude rited of most a\n",
      "xvlw homors and sulres throue with ib stacks differlaney ung the brium in she orf\n",
      "mulit been a plort lests rites the sges an inforoductions yoryy world memoused am\n",
      "================================================================================\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 6100: 1.839475 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 6200: 1.852397 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 6300: 1.832353 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 6400: 1.841567 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 6500: 1.848558 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 6600: 1.853359 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 6700: 1.838514 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 6800: 1.813291 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 6900: 1.841398 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 7000: 1.863734 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.68\n",
      "================================================================================\n",
      "wb by s had seant g in small lations in name the minicde xas in resessigny d manh\n",
      "jzp afrom in edic liane in the to or to is discused of the semvemlen a cocirther \n",
      "endueltmil lowing of hustriptionation of sher to centrish and of from bauja pign \n",
      "mber fishey r contems the crims is sir the mmeral fulland eight stolling bord sta\n",
      "oker of h to that a sing the scused to prics and crificate as saccommib rair of t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.84\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "            \n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "            \n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]})\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "keep_prob = 0.75 # dropout probability\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size**2, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)    \n",
    "    i_embed = tf.nn.dropout(tf.nn.embedding_lookup(embeddings, bigram_index),\n",
    "                            keep_prob)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    output = tf.nn.dropout(output, keep_prob)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = []\n",
    "    \n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "\n",
    "  sample_input_index = tf.argmax(sample_input[0], dimension=1) + \\\n",
    "                                vocabulary_size * tf.argmax(sample_input[1], dimension=1)    \n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, sample_input_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.307505 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.32\n",
      "================================================================================\n",
      "jxr wnonhtefme iemebtutyhyvzbropbr  eyugeetofy fdh erroheded r otncoyghawlqd utdy\n",
      "oyfem hu lza fsm d yteeu hneeeorakxmejbtdrjnme ecnr yytmqabes uxtxqfqep zthnzi  k\n",
      "hsmnsjb d ttilterwaeftwg tfb ka ptg qkdconbk fqzraaerovne  vq eoxhhpemeq   m r  b\n",
      "zrauussi elth gtmeaegqnlzujrgoha pp  gvtqegmebv ebnre  cebtimke fxerath  r fvqsma\n",
      "rwrcg te gerfj t hxvr diqozf hkzleew d r vyujferet  einassud py ah  uenahn nxyey \n",
      "================================================================================\n",
      "Validation set perplexity: 20.39\n",
      "Average loss at step 100: 2.448665 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.19\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 200: 2.169880 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.07\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 300: 2.101429 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 400: 2.081476 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 500: 2.031622 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 600: 2.033586 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 700: 2.017086 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 800: 1.985387 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 900: 1.990190 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 1000: 1.970223 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "================================================================================\n",
      "wws son worn norold moter three insing a the invers te to to dift the it to eclan\n",
      "yz s dedoes one nine eight fart the nest wulting impion the retive the liton zero\n",
      "fb or becaning aftertast on his of be of as gerrian the freiver the bot harbothen\n",
      "sh and the con and to which wher a s siker mesence whin the riihort a aix that th\n",
      "pbut aq invorsi on the rege daom rell resunirone resefls to arough ad pty oping i\n",
      "================================================================================\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 1100: 1.964620 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.71\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 1200: 1.981163 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 1300: 1.944033 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1400: 1.974049 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 1500: 1.942440 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 1600: 1.924989 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 1700: 1.911426 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 1800: 1.918395 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1900: 1.936285 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.18\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 2000: 1.909708 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "================================================================================\n",
      "sginally plish the mach and ala a attion the with the take i see r obir the he in\n",
      "cbssuus in m of pares of a not the four skills the buction the as hiles the hip h\n",
      "xdaeank arlish the controbary the and to howelvynly cresimitis have of to how nin\n",
      "cwith oon the pioging aro lana memiled us in six famelity one least comotbas in o\n",
      "oemerinspon wat state viney or the rivey molul in gurch aximdress though is indec\n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2100: 1.933407 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2200: 1.934584 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 2300: 1.931914 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.10\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 2400: 1.921200 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 2500: 1.918400 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 2600: 1.910341 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2700: 1.911321 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 2800: 1.903415 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 2900: 1.892496 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3000: 1.889034 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "================================================================================\n",
      " exge rais thant in and capada spein aner sevel of the ry deartic catte a stell a\n",
      "zero mill and anis chare radly fromnly genal now the in bond by cament the consiv\n",
      " kaffectainly chrian terpho nummaled good in back integial ariaudans one sh any p\n",
      "xw and movily media the main charess englancement mager recomicade modell in new \n",
      "chight eristemal one nine five nine unter the posent the prishe used it the pemed\n",
      "================================================================================\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3100: 1.894822 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 3200: 1.879752 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 3300: 1.900938 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3400: 1.867840 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3500: 1.895289 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 3600: 1.889025 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 3700: 1.853911 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 3800: 1.873290 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 3900: 1.871917 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 4000: 1.914968 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "================================================================================\n",
      "cn division diseon of mill of mary ravynoxeus diagesjewed mlavioning hance chare \n",
      "qkings for exprottic refial grows somebergy that both muwill estangess succepted \n",
      "vnation of the contre dis surbath granded two zero nine accuss in the playintive \n",
      "d in are agains the that main a lifaries prosbouswinreed inthen anguands celle un\n",
      "tbond with sevile in the his stant also usion he eolla systiectuoudpment on six n\n",
      "================================================================================\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 4100: 1.879598 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 4200: 1.885979 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 4300: 1.889613 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4400: 1.849049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4500: 1.863312 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 4600: 1.864485 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 4700: 1.869510 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4800: 1.863667 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 4900: 1.862177 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5000: 1.876483 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.12\n",
      "================================================================================\n",
      "uoand to in to caputer prece publith icificative is difficishis of re seng and ve\n",
      "rled lewinly the birally mar in sophave with and slide the poectors nampord intil\n",
      "me fibrairstor other the in proped a doper such firrefencian one nine micrelation\n",
      "iyally the balian after arrorgy they compiratility werd of for as with mamuseorip\n",
      "gfn the naman yeuopular as the namuch sinseive can his the insion engilated batli\n",
      "================================================================================\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 5100: 1.843892 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 5200: 1.845136 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 5300: 1.845036 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 5400: 1.838777 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 5500: 1.848269 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 5600: 1.839534 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 5700: 1.829144 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 5800: 1.847275 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 5900: 1.861653 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 6000: 1.829321 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.07\n",
      "================================================================================\n",
      "xk or iuness carroes hiles a lod cason same univident part givated their dic have\n",
      " from ceollely roment clunk altain modes line large ridposains is five six sben s\n",
      "lverly was sto westript eare is and other to keas upaids falso in asselled basgan\n",
      "through have drefortying mad ax who long humockhony to a solunced in saurth ut ve\n",
      " ya nellike under in beforecg this as fors p exparrunitic avidence crutally the b\n",
      "================================================================================\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 6100: 1.831707 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 6200: 1.813565 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 6300: 1.840049 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6400: 1.809467 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 6500: 1.807171 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6600: 1.831691 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 6700: 1.796407 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6800: 1.783888 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6900: 1.785620 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 7000: 1.821770 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "dworld high welled poblezramong enctoriveting fivaters one zero zero nine that ct\n",
      "gy a lywels be johnins into boad to dated avisn is tuniver hailpgdically partible\n",
      "opvny u izantchetis herough in light to three pb   forming one nine one nine eigh\n",
      "ium a publics year had which a femarmations earmor eone nye marce of relember and\n",
      "a as right at or sign the da schorent as conformer deslace gestew and a gibius of\n",
      "================================================================================\n",
      "Validation set perplexity: 6.53\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "            \n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "            \n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]})\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
