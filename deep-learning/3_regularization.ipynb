{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  # regularization parameter\n",
    "  tf_beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + tf_beta * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.513441\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 16.8%\n",
      "Minibatch loss at step 500: 2.659864\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 1000: 1.689404\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 1500: 1.042121\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 2000: 0.907482\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 2500: 0.863228\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 3000: 1.147194\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 77.9%\n",
      "Test accuracy: 83.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "beta_vals = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_vals = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_beta : beta_val}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "      accuracy_vals.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF4CAYAAAAWmIDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVflATX0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b1c597910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogx(beta_vals, accuracy_vals)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = image_size * image_size\n",
    "n_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  # regularization parameter\n",
    "  tf_beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = {\n",
    "    'h': tf.Variable(tf.truncated_normal([n_input, n_hidden])),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, num_labels]))\n",
    "  }\n",
    "  biases = {\n",
    "    'h': tf.Variable(tf.zeros([n_hidden])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "  \n",
    "  # Training computation.\n",
    "  activ1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights['h']) + biases['h'])\n",
    "  logits = tf.matmul(activ1, weights['out']) + biases['out']\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "    tf_beta * (tf.nn.l2_loss(weights['h']) + tf.nn.l2_loss(weights['out']))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights['h']) + biases['h']),\n",
    "             weights['out']) + biases['out'])\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights['h']) + biases['h']),\n",
    "             weights['out']) + biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 645.950195\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 23.7%\n",
      "Minibatch loss at step 500: 194.122925\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 1000: 113.928192\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 1500: 68.173462\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2000: 42.600380\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 25.303392\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 3000: 15.704386\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.4%\n",
      "Test accuracy: 89.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 641.647705\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 23.2%\n",
      "Minibatch loss at step 10: 311.368805\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.1%\n",
      "Minibatch loss at step 20: 308.267944\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 30: 305.200378\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 40: 302.163422\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 50: 299.156036\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 60: 296.178802\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 70: 293.231415\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 80: 290.311920\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 90: 287.423157\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 100: 284.562775\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Test accuracy: 68.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = image_size * image_size\n",
    "n_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # dropout probability\n",
    "  tf_keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "  # Variables.\n",
    "  weights = {\n",
    "    'h': tf.Variable(tf.truncated_normal([n_input, n_hidden])),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, num_labels]))\n",
    "  }\n",
    "  biases = {\n",
    "    'h': tf.Variable(tf.zeros([n_hidden])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "    \n",
    "  # Training computation.\n",
    "  activ = tf.nn.relu(tf.matmul(tf_train_dataset, weights['h']) + biases['h'])\n",
    "  drop = tf.nn.dropout(activ, tf_keep_prob)\n",
    "\n",
    "  logits = tf.matmul(drop, weights['out']) + biases['out']\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights['h']) + biases['h']),\n",
    "             weights['out']) + biases['out'])\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights['h']) + biases['h']),\n",
    "             weights['out']) + biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 462.473816\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 32.6%\n",
      "Minibatch loss at step 10: 6.766976\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 20: 0.667100\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 30: 0.000007\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 40: 2.408754\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 50: 0.233007\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 70: 0.355141\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 100: 0.420271\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.6%\n",
      "Test accuracy: 72.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_keep_prob : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = image_size * image_size\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # dropout probability\n",
    "  tf_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "  # learning_rate\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  tf_starter_learning_rate = tf.placeholder(tf.float32)\n",
    "  tf_decay_steps = tf.placeholder(tf.float32)\n",
    "  learning_rate = tf.train.exponential_decay(tf_starter_learning_rate,\n",
    "                                             global_step, tf_decay_steps, 0.99999, staircase=True)\n",
    "    \n",
    "  # Variables.\n",
    "  n_hidden = {\n",
    "    'h1': 1024,\n",
    "    'h2': 512,\n",
    "    'h3': 512,\n",
    "    'h4': 256\n",
    "  }\n",
    "\n",
    "  weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden['h1']],\n",
    "                                         stddev=np.sqrt(2.0 / n_input))),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden['h1'], n_hidden['h2']],\n",
    "                                         stddev=np.sqrt(2.0 / n_hidden['h1']))),\n",
    "    'h3': tf.Variable(tf.truncated_normal([n_hidden['h2'], n_hidden['h3']],\n",
    "                                         stddev=np.sqrt(2.0 / n_hidden['h2']))),\n",
    "    'h4': tf.Variable(tf.truncated_normal([n_hidden['h3'], n_hidden['h4']],\n",
    "                                         stddev=np.sqrt(2.0 / n_hidden['h3']))),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden['h4'], num_labels],\n",
    "                                         stddev=np.sqrt(2.0 / n_hidden['h4'])))\n",
    "  }\n",
    "  biases = {\n",
    "    'h1': tf.Variable(tf.zeros([n_hidden['h1']])),\n",
    "    'h2': tf.Variable(tf.zeros([n_hidden['h2']])),\n",
    "    'h3': tf.Variable(tf.zeros([n_hidden['h3']])),\n",
    "    'h4': tf.Variable(tf.zeros([n_hidden['h4']])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "    \n",
    "  # Training computation.\n",
    "  activ1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights['h1']) + biases['h1'])\n",
    "  drop1 = tf.nn.dropout(activ1, tf_keep_prob)\n",
    "      \n",
    "  activ2 = tf.nn.relu(tf.matmul(drop1, weights['h2']) + biases['h2'])\n",
    "  drop2 = tf.nn.dropout(activ2, tf_keep_prob)\n",
    "    \n",
    "  activ3 = tf.nn.relu(tf.matmul(drop2, weights['h3']) + biases['h3'])\n",
    "  drop3 = tf.nn.dropout(activ3, tf_keep_prob)\n",
    "    \n",
    "  activ4 = tf.nn.relu(tf.matmul(drop3, weights['h4']) + biases['h4'])\n",
    "  drop4 = tf.nn.dropout(activ4, tf_keep_prob)\n",
    "    \n",
    "  logits = tf.matmul(drop4, weights['out']) + biases['out']\n",
    "\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(tf_starter_learning_rate).minimize(loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.AdamOptimizer(tf_starter_learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  valid_activ1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights['h1']) + biases['h1'])\n",
    "  valid_activ2 = tf.nn.relu(tf.matmul(valid_activ1, weights['h2']) + biases['h2'])\n",
    "  valid_activ3 = tf.nn.relu(tf.matmul(valid_activ2, weights['h3']) + biases['h3'])\n",
    "  valid_activ4 = tf.nn.relu(tf.matmul(valid_activ3, weights['h4']) + biases['h4'])\n",
    "    \n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_activ4, weights['out']) + biases['out'])\n",
    "\n",
    "  test_activ1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights['h1']) + biases['h1'])\n",
    "  test_activ2 = tf.nn.relu(tf.matmul(test_activ1, weights['h2']) + biases['h2'])\n",
    "  test_activ3 = tf.nn.relu(tf.matmul(test_activ2, weights['h3']) + biases['h3'])\n",
    "  test_activ4 = tf.nn.relu(tf.matmul(test_activ3, weights['h4']) + biases['h4'])\n",
    "\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_activ4, weights['out']) + biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.960402\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 11.3%\n",
      "Minibatch loss at step 500: 0.527599\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1000: 0.791469\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1500: 0.584694\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2000: 0.553001\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2500: 0.653467\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3000: 0.603536\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3500: 0.533797\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4000: 0.440643\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4500: 0.466943\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5000: 0.447857\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5500: 0.526850\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 6000: 0.502010\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 6500: 0.338800\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 7000: 0.475074\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 7500: 0.420205\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 8000: 0.357102\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 8500: 0.480670\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 9000: 0.421983\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 9500: 0.257531\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 10000: 0.425935\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 10500: 0.367317\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 11000: 0.411885\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 11500: 0.636660\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12000: 0.379430\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 12500: 0.357080\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 13000: 0.300274\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 13500: 0.496632\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 14000: 0.500889\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 14500: 0.345319\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 15000: 0.327925\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 15500: 0.340554\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 16000: 0.461743\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 16500: 0.493924\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 17000: 0.415509\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 17500: 0.371550\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 18000: 0.272878\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 18500: 0.231430\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 19000: 0.379078\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 19500: 0.351198\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 20000: 0.257000\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 20500: 0.481128\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 21000: 0.328026\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 21500: 0.470555\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 22000: 0.433522\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 22500: 0.383947\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 23000: 0.428794\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 23500: 0.325491\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 24000: 0.354009\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 24500: 0.231736\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 25000: 0.230163\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 25500: 0.284980\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 26000: 0.391539\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 26500: 0.381743\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 27000: 0.391740\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 27500: 0.274157\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 28000: 0.360835\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 28500: 0.330311\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 29000: 0.484871\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 29500: 0.288793\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 30000: 0.491351\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 30500: 0.230243\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 31000: 0.356764\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 31500: 0.285835\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 32000: 0.347913\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 32500: 0.217728\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 33000: 0.327877\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 33500: 0.318481\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 34000: 0.470310\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 34500: 0.157770\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 35000: 0.167760\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 35500: 0.287668\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 36000: 0.351825\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 36500: 0.433762\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 37000: 0.434793\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 37500: 0.313813\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 38000: 0.350485\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 38500: 0.423133\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 39000: 0.351911\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 39500: 0.206516\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 40000: 0.252710\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 40500: 0.277615\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 41000: 0.202387\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 41500: 0.169362\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 42000: 0.231662\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 42500: 0.287586\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 43000: 0.109918\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 43500: 0.351158\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 44000: 0.258807\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 44500: 0.250083\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 45000: 0.209468\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 45500: 0.256858\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 46000: 0.259566\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 46500: 0.218907\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 47000: 0.339971\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 47500: 0.299409\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 48000: 0.248654\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 48500: 0.344729\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 49000: 0.254021\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 49500: 0.336150\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 50000: 0.407279\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 50500: 0.258941\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 51000: 0.308568\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 51500: 0.319108\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 52000: 0.331004\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 52500: 0.293758\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 53000: 0.231032\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 53500: 0.302332\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 54000: 0.423304\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 54500: 0.361358\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 55000: 0.160332\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 55500: 0.295248\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 56000: 0.263329\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 56500: 0.217706\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 57000: 0.390380\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 57500: 0.336057\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 58000: 0.238046\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 58500: 0.304800\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 59000: 0.267787\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 59500: 0.328410\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 60000: 0.132206\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 60500: 0.184748\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 61000: 0.235498\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 61500: 0.219548\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 62000: 0.310670\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 62500: 0.219723\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 63000: 0.247528\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 63500: 0.251682\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 64000: 0.270503\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 64500: 0.299370\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 65000: 0.194496\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 65500: 0.194828\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 66000: 0.357769\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 66500: 0.157397\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 67000: 0.222747\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 67500: 0.178767\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 68000: 0.200055\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 68500: 0.192381\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 69000: 0.164713\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 69500: 0.280692\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 70000: 0.309423\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 70500: 0.267193\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 71000: 0.159689\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 71500: 0.403149\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 72000: 0.345580\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 72500: 0.246486\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 73000: 0.307679\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 73500: 0.348018\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 74000: 0.218119\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 74500: 0.224477\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 75000: 0.175712\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 75500: 0.245197\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 76000: 0.260544\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 76500: 0.378793\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 77000: 0.407514\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 77500: 0.285822\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 78000: 0.226049\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 78500: 0.176535\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 79000: 0.344606\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 79500: 0.129747\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 80000: 0.194153\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 80500: 0.132891\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 81000: 0.198735\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 81500: 0.194993\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 82000: 0.207599\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 82500: 0.172473\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 83000: 0.302080\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 83500: 0.284828\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 84000: 0.136636\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 84500: 0.183381\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 85000: 0.363199\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 85500: 0.205991\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 86000: 0.397488\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 86500: 0.269574\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 87000: 0.373223\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 87500: 0.197944\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 88000: 0.309521\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 88500: 0.238675\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 89000: 0.145496\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 89500: 0.266496\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 90000: 0.156254\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 90500: 0.272865\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 91000: 0.223919\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 91500: 0.231370\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 92000: 0.389633\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 92500: 0.287836\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 93000: 0.164444\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 93500: 0.232096\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 94000: 0.275339\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 94500: 0.208193\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 95000: 0.152706\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 95500: 0.368490\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 96000: 0.161020\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 96500: 0.186435\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 97000: 0.249020\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 97500: 0.208331\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 98000: 0.187337\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 98500: 0.196167\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 99000: 0.224835\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 99500: 0.184929\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 100000: 0.195730\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 100500: 0.185582\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 101000: 0.210907\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 101500: 0.338498\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 102000: 0.223345\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 102500: 0.381405\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 103000: 0.190997\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 103500: 0.115655\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 104000: 0.253700\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 104500: 0.275453\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 105000: 0.301686\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 105500: 0.164716\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 106000: 0.186981\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 106500: 0.243000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 107000: 0.232822\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 107500: 0.154037\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 108000: 0.280657\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 108500: 0.142998\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 109000: 0.242841\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 109500: 0.203940\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 110000: 0.258999\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 110500: 0.239112\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 111000: 0.283699\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 111500: 0.243310\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 112000: 0.179061\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 112500: 0.138919\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 113000: 0.321343\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 113500: 0.263525\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 114000: 0.280044\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 114500: 0.193927\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 115000: 0.141521\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 115500: 0.256648\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 116000: 0.214700\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 116500: 0.118560\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 117000: 0.251069\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 117500: 0.199903\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 118000: 0.181512\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 118500: 0.180742\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 119000: 0.207806\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 119500: 0.195612\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 120000: 0.162944\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 120500: 0.266427\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 121000: 0.146101\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 121500: 0.279886\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 122000: 0.178897\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 122500: 0.125807\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 123000: 0.288138\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 123500: 0.134376\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 124000: 0.168176\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 124500: 0.150857\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 125000: 0.189860\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 125500: 0.195230\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 126000: 0.090536\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 126500: 0.217432\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 127000: 0.212803\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 127500: 0.256445\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 128000: 0.223402\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 128500: 0.236862\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 129000: 0.155408\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 129500: 0.133856\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 130000: 0.348450\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 130500: 0.205090\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 131000: 0.266454\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 131500: 0.232941\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 132000: 0.137601\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 132500: 0.247822\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 133000: 0.300402\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 133500: 0.158073\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 134000: 0.122311\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 134500: 0.207605\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 135000: 0.255693\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 135500: 0.197066\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 136000: 0.127193\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 136500: 0.172937\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 137000: 0.265870\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 137500: 0.240977\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 138000: 0.128197\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 138500: 0.179358\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 139000: 0.176963\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 139500: 0.173683\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 140000: 0.174511\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 140500: 0.203389\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 141000: 0.208359\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 141500: 0.234847\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 142000: 0.164157\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 142500: 0.106001\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 143000: 0.194495\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 143500: 0.158032\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 144000: 0.152459\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 144500: 0.109693\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 145000: 0.099480\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 145500: 0.365810\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 146000: 0.225469\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 146500: 0.122643\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 147000: 0.067103\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 147500: 0.175139\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 148000: 0.270499\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 148500: 0.207719\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 149000: 0.273975\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 149500: 0.161418\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 150000: 0.249891\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Test accuracy: 96.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 150001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,\n",
    "                 tf_keep_prob : .5, tf_starter_learning_rate : 0.1, tf_decay_steps: 1}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
